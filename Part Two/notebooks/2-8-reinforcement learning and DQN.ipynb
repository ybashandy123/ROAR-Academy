{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0937657",
   "metadata": {},
   "source": [
    "**Learning Python for Artificial Intelligence and Data Science**\n",
    "\n",
    "**Lecture: Reinforcement Learning and Deep Q-Network (DQN)**\n",
    "\n",
    "**By Allen Y. Yang, PhD**\n",
    "\n",
    "(c) Copyright Intelligent Racing Inc., 2020-2024. All rights reserved. Materials may NOT be distributed or used for any commercial purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42058fe2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Reinforcement Learning (RL) is a powerful machine learning paradigm inspired by how agents learn to act in an environment through trial and error. In this notebook, we explore the core concepts of RL, the Bellman equation, and Deep Q-Networks (DQN). We will train an agent using DQN on the classic CartPole-v1 environment from OpenAI's `gym`.\n",
    "\n",
    "### What Makes Reinforcement Learning Special?\n",
    "\n",
    "Unlike supervised learning where we have labeled data (like pictures of cats labeled \"cat\"), or unsupervised learning where we find patterns in data, reinforcement learning is about **learning from interaction**. Think of it like learning to ride a bicycle:\n",
    "\n",
    "- You don't have a dataset of \"correct\" bicycle movements\n",
    "- You learn by trying, falling, and adjusting\n",
    "- You get feedback (rewards) - staying balanced feels good, falling hurts\n",
    "- Over time, you develop a strategy (policy) for riding successfully\n",
    "\n",
    "### Real-World Examples of Reinforcement Learning\n",
    "\n",
    "1. **Video Games**: Game AI that learns to play chess, Go, or video games\n",
    "2. **Robotics**: Robots learning to walk, grasp objects, or navigate\n",
    "3. **Finance**: Trading algorithms that learn optimal investment strategies\n",
    "4. **Healthcare**: Treatment planning that adapts to patient responses\n",
    "5. **Autonomous Vehicles**: Learning to drive safely in various conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ba712",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Basics\n",
    "\n",
    "### The RL Framework\n",
    "\n",
    "Imagine you're playing a video game. At each moment:\n",
    "1. You see the current game screen (the **state**)\n",
    "2. You decide what button to press (the **action**)\n",
    "3. The game responds with points and a new screen (the **reward** and **next state**)\n",
    "\n",
    "This cycle continues until the game ends. RL algorithms learn to maximize the total score!\n",
    "\n",
    "### Key Components\n",
    "\n",
    "An RL problem is typically modeled as a **Markov Decision Process (MDP)**. The agent interacts with an environment over discrete time steps.\n",
    "\n",
    "- **State (s)**: Current situation of the agent.\n",
    "- **Action (a)**: Choices available to the agent.\n",
    "- **Reward (r)**: Feedback from the environment.\n",
    "- **Policy (œÄ)**: Strategy used by the agent.\n",
    "- **Q-Value**: Expected future rewards given a state and action.\n",
    "\n",
    "### Understanding States, Actions, and Rewards\n",
    "\n",
    "Let's use a simple example: **Teaching a Dog to Fetch**\n",
    "\n",
    "- **States**: Where is the ball? Where is the dog? Is the dog holding the ball?\n",
    "- **Actions**: Run forward, turn left/right, pick up ball, drop ball\n",
    "- **Rewards**: \n",
    "  - +10 for bringing ball back to owner\n",
    "  - +1 for picking up the ball\n",
    "  - -0.1 for each time step (encourages efficiency)\n",
    "  - -5 for running into obstacles\n",
    "\n",
    "The dog (agent) learns which actions in which states lead to the most treats (rewards)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markov_process",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "### What is the Markov Property?\n",
    "\n",
    "The Markov property states that **\"the future depends only on the present, not the past.\"** In other words, if you know the current state, you have all the information needed to predict what happens next.\n",
    "\n",
    "### Simple Example: Weather Prediction\n",
    "\n",
    "Imagine a simplified weather system with only three states:\n",
    "- ‚òÄÔ∏è Sunny\n",
    "- ‚òÅÔ∏è Cloudy \n",
    "- üåßÔ∏è Rainy\n",
    "\n",
    "With the Markov property, tomorrow's weather depends ONLY on today's weather, not on last week's weather pattern.\n",
    "\n",
    "```\n",
    "Today is Sunny ‚Üí Tomorrow: 70% Sunny, 20% Cloudy, 10% Rainy\n",
    "Today is Cloudy ‚Üí Tomorrow: 30% Sunny, 40% Cloudy, 30% Rainy\n",
    "Today is Rainy ‚Üí Tomorrow: 20% Sunny, 30% Cloudy, 50% Rainy\n",
    "```\n",
    "\n",
    "### Why is this Important for RL?\n",
    "\n",
    "In our CartPole example:\n",
    "- The **state** includes: cart position, cart velocity, pole angle, pole velocity\n",
    "- This state contains ALL information needed to decide the next action\n",
    "- We don't need to know how the pole got to this angle - just the current state!\n",
    "\n",
    "### MDP Components\n",
    "\n",
    "A Markov Decision Process consists of:\n",
    "\n",
    "1. **S**: Set of all possible states\n",
    "2. **A**: Set of all possible actions\n",
    "3. **P(s'|s,a)**: Probability of reaching state s' from state s after taking action a\n",
    "4. **R(s,a,s')**: Reward received for going from state s to s' via action a\n",
    "5. **Œ≥ (gamma)**: Discount factor (0 ‚â§ Œ≥ ‚â§ 1) - how much we value future rewards\n",
    "\n",
    "### The Discount Factor (Œ≥)\n",
    "\n",
    "Why do we discount future rewards? Consider this choice:\n",
    "- Option A: Get $100 today\n",
    "- Option B: Get $100 in one year\n",
    "\n",
    "Most people prefer Option A! In RL:\n",
    "- Œ≥ = 0: Only care about immediate rewards (very short-sighted)\n",
    "- Œ≥ = 0.9: Future rewards are worth 90% of immediate rewards\n",
    "- Œ≥ = 1: Future rewards are just as valuable as immediate rewards\n",
    "\n",
    "In our code, we use Œ≥ = 0.95, meaning rewards 10 steps in the future are worth about 60% of immediate rewards (0.95^10 ‚âà 0.60)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q_function_section",
   "metadata": {},
   "source": [
    "## The Q-Function (Action-Value Function)\n",
    "\n",
    "### What is a Q-Value?\n",
    "\n",
    "The Q-value answers the question: **\"If I'm in state s and take action a, what's my expected total future reward?\"**\n",
    "\n",
    "Think of it as a crystal ball that tells you the long-term value of each action!\n",
    "\n",
    "### Intuitive Example: Choosing a Route to School\n",
    "\n",
    "You're at an intersection (state) and can go:\n",
    "- **Left** (action 1): Usually fast, but sometimes has traffic\n",
    "- **Right** (action 2): Always slow but reliable\n",
    "- **Straight** (action 3): Medium speed, occasional construction\n",
    "\n",
    "The Q-values might be:\n",
    "- Q(intersection, left) = -15 minutes (average time)\n",
    "- Q(intersection, right) = -20 minutes\n",
    "- Q(intersection, straight) = -17 minutes\n",
    "\n",
    "The best action is 'left' because it has the highest Q-value (least negative = shortest time)!\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "The Q-function is defined as:\n",
    "\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}[R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ... | S_t = s, A_t = a]$$\n",
    "\n",
    "Where:\n",
    "- $Q^\\pi(s,a)$ is the Q-value for state s and action a under policy œÄ\n",
    "- $\\mathbb{E}$ means \"expected value\" (average)\n",
    "- $R_t, R_{t+1}, ...$ are future rewards\n",
    "- $\\gamma$ is the discount factor\n",
    "\n",
    "### Why Q-Values are Powerful\n",
    "\n",
    "Once we know all Q-values, choosing actions is easy:\n",
    "- Just pick the action with the highest Q-value!\n",
    "- This gives us the optimal policy: $\\pi^*(s) = \\arg\\max_a Q^*(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bellman_equation_section",
   "metadata": {},
   "source": [
    "## The Bellman Equation\n",
    "\n",
    "The Bellman Equation provides a recursive decomposition for computing Q-values:\n",
    "\n",
    "$$Q(s_t, a) = r(s_t, a) + \\gamma \\max_{a'} Q(s_{t+1}, a')$$\n",
    "\n",
    "Where $\\gamma$ is the discount factor.\n",
    "\n",
    "### Understanding the Bellman Equation\n",
    "\n",
    "This equation says: **\"The value of an action = immediate reward + discounted value of the best future action\"**\n",
    "\n",
    "### Step-by-Step Example: Treasure Hunt Game\n",
    "\n",
    "Imagine a simple game on a 3x1 grid:\n",
    "```\n",
    "[Start] ‚Üí [Middle] ‚Üí [Treasure!]\n",
    "```\n",
    "\n",
    "- Being at Treasure gives +10 reward\n",
    "- Each move costs -1 (time penalty)\n",
    "- Discount factor Œ≥ = 0.9\n",
    "\n",
    "Let's calculate Q-values backward:\n",
    "\n",
    "1. **At Middle position:**\n",
    "   - Q(Middle, go_right) = -1 + 0.9 √ó 10 = 8\n",
    "   - Q(Middle, go_left) = -1 + 0.9 √ó 0 = -1\n",
    "\n",
    "2. **At Start position:**\n",
    "   - Q(Start, go_right) = -1 + 0.9 √ó max(8, -1) = -1 + 7.2 = 6.2\n",
    "\n",
    "This tells us: Starting from 'Start', the value of going right is 6.2!\n",
    "\n",
    "### Why is this Recursive?\n",
    "\n",
    "Notice how we used the Q-value of the next state to calculate the current Q-value. This is the power of the Bellman equation - it breaks down a complex problem into simpler subproblems!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q_learning_section",
   "metadata": {},
   "source": [
    "## From Q-Learning to Deep Q-Networks\n",
    "\n",
    "### Traditional Q-Learning\n",
    "\n",
    "In simple environments, we could store Q-values in a table:\n",
    "\n",
    "| State | Action | Q-Value |\n",
    "|-------|--------|----------|\n",
    "| State1 | Left   | 5.2     |\n",
    "| State1 | Right  | 3.1     |\n",
    "| State2 | Left   | 7.8     |\n",
    "| ...    | ...    | ...     |\n",
    "\n",
    "But what if we have millions of states? Or continuous states like CartPole (pole angle can be any value)?\n",
    "\n",
    "### Enter Neural Networks!\n",
    "\n",
    "Instead of a table, we use a neural network to approximate Q-values:\n",
    "- **Input**: Current state (e.g., [cart_position, cart_velocity, pole_angle, pole_velocity])\n",
    "- **Output**: Q-value for each possible action (e.g., [Q(state, left), Q(state, right)])\n",
    "\n",
    "This is a **Deep Q-Network (DQN)**!\n",
    "\n",
    "### How DQN Learns\n",
    "\n",
    "1. **Experience**: Agent tries actions and stores experiences (state, action, reward, next_state)\n",
    "2. **Replay**: Randomly sample past experiences to learn from\n",
    "3. **Update**: Adjust neural network weights to better predict Q-values\n",
    "4. **Explore vs Exploit**: Sometimes try random actions (explore) vs choosing best known action (exploit)\n",
    "\n",
    "### The Exploration-Exploitation Trade-off\n",
    "\n",
    "Imagine you're at a new ice cream shop:\n",
    "- **Exploit**: Always order vanilla (you know it's good)\n",
    "- **Explore**: Try new flavors (might find something better!)\n",
    "\n",
    "In our code:\n",
    "- Œµ (epsilon) = probability of exploring (trying random action)\n",
    "- Starts at 1.0 (100% exploration) and decays to 0.01 (1% exploration)\n",
    "- This ensures we explore early but exploit our knowledge later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cartpole_explanation",
   "metadata": {},
   "source": [
    "## The CartPole Environment\n",
    "\n",
    "### What is CartPole?\n",
    "\n",
    "CartPole is like balancing a broomstick on your hand:\n",
    "- A cart can move left or right on a track\n",
    "- A pole is attached to the cart with a hinge\n",
    "- Goal: Keep the pole upright as long as possible!\n",
    "\n",
    "### State Space (What the Agent Sees)\n",
    "\n",
    "The state consists of 4 numbers:\n",
    "1. **Cart Position**: How far left/right is the cart? (-2.4 to 2.4)\n",
    "2. **Cart Velocity**: How fast is the cart moving?\n",
    "3. **Pole Angle**: How tilted is the pole? (in radians)\n",
    "4. **Pole Angular Velocity**: How fast is the pole rotating?\n",
    "\n",
    "### Action Space (What the Agent Can Do)\n",
    "\n",
    "Only 2 actions:\n",
    "- 0: Push cart to the LEFT\n",
    "- 1: Push cart to the RIGHT\n",
    "\n",
    "### Rewards\n",
    "\n",
    "- +1 for every timestep the pole remains upright\n",
    "- Episode ends (and we give -10 penalty in our code) if:\n",
    "  - Pole angle exceeds ¬±12¬∞\n",
    "  - Cart moves beyond ¬±2.4 units\n",
    "  - 500 timesteps pass (solved!)\n",
    "\n",
    "### Why is CartPole Challenging?\n",
    "\n",
    "- **Non-linear dynamics**: Small changes can have big effects\n",
    "- **Delayed consequences**: Moving left now might cause a fall later\n",
    "- **Continuous states**: Infinite possible positions/angles\n",
    "- **Balance trade-off**: Must balance pole AND keep cart centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627721b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time as time_module  # Rename to avoid conflict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "EPISODES = 100\n",
    "RENDER_EVERY = 10  # Render every N episodes to see progress\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "# Try different ways to create the environment\n",
    "env = None\n",
    "render_method = None\n",
    "\n",
    "# Method 1: Try with old API\n",
    "try:\n",
    "    env = gym.make('CartPole-v1')\n",
    "    env.reset()\n",
    "    env.render(mode='rgb_array')\n",
    "    render_method = 'old_rgb'\n",
    "    print(\"Using old render API with rgb_array mode\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Method 2: Try default render\n",
    "if env is None or render_method is None:\n",
    "    try:\n",
    "        env = gym.make('CartPole-v1')\n",
    "        env.reset()\n",
    "        env.render()\n",
    "        render_method = 'old_default'\n",
    "        print(\"Using old render API with default mode\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Method 3: Just create environment without render\n",
    "if env is None:\n",
    "    env = gym.make('CartPole-v1')\n",
    "    render_method = 'manual'\n",
    "    print(\"Created environment, will attempt manual visualization\")\n",
    "\n",
    "print(render_method)\n",
    "# Test reset to check API version\n",
    "state_or_tuple = env.reset()\n",
    "if isinstance(state_or_tuple, tuple):\n",
    "    state, _ = state_or_tuple\n",
    "    uses_new_api = True\n",
    "else:\n",
    "    state = state_or_tuple\n",
    "    uses_new_api = False\n",
    "\n",
    "print(f\"API version: {'new' if uses_new_api else 'old'}\")\n",
    "\n",
    "# Function to get frame based on render method\n",
    "def get_frame(env, render_method):\n",
    "    try:\n",
    "        if render_method == 'old_rgb':\n",
    "            return env.render(mode='rgb_array')\n",
    "        elif render_method == 'old_default':\n",
    "            # Try to get rgb_array even if not default\n",
    "            try:\n",
    "                return env.render(mode='rgb_array')\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Manual visualization fallback\n",
    "def draw_cartpole_state(episode, state, average):\n",
    "    \"\"\"Manually draw the cartpole state\"\"\"\n",
    "    cart_pos = state[0]\n",
    "    pole_angle = state[2]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Set up the plot\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-1, 2)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Draw track\n",
    "    ax.plot([-2.4, 2.4], [0, 0], 'k-', linewidth=2)\n",
    "    ax.plot([-2.4, -2.4], [-0.05, 0.05], 'r-', linewidth=4)\n",
    "    ax.plot([2.4, 2.4], [-0.05, 0.05], 'r-', linewidth=4)\n",
    "    \n",
    "    # Draw cart\n",
    "    cart_width = 0.3\n",
    "    cart_height = 0.2\n",
    "    cart = plt.Rectangle((cart_pos - cart_width/2, -cart_height/2), \n",
    "                        cart_width, cart_height, \n",
    "                        fill=True, color='blue')\n",
    "    ax.add_patch(cart)\n",
    "    \n",
    "    # Draw pole\n",
    "    pole_length = 1.0\n",
    "    pole_end_x = cart_pos + pole_length * np.sin(pole_angle)\n",
    "    pole_end_y = pole_length * np.cos(pole_angle)\n",
    "    ax.plot([cart_pos, pole_end_x], [0, pole_end_y], 'brown', linewidth=8)\n",
    "    \n",
    "    # Add pole joint\n",
    "    circle = plt.Circle((cart_pos, 0), 0.05, color='black')\n",
    "    ax.add_patch(circle)\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title(f'Episode: {episode}, Angle: {np.degrees(pole_angle):.1f}¬∞, Average Reward: {average}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Height')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Run one episode\n",
    "angle = state[2]\n",
    "angle_velocity = state[3]\n",
    "integral = 0.0\n",
    "prev_error = angle\n",
    "batch_size = 32\n",
    "scores = []  # Store scores for plotting\n",
    "\n",
    "\n",
    "print(\"\\nRunning CartPole with PID Controller...\")\n",
    "print(\"Watch the game below:\\n\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "for e in range(EPISODES):\n",
    "\n",
    "    # Test reset to check API version\n",
    "    state_or_tuple = env.reset()\n",
    "    if uses_new_api:\n",
    "        state, _ = state_or_tuple\n",
    "    else:\n",
    "        state = state_or_tuple\n",
    "\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    # Determine if we should render this episode\n",
    "    render_this_episode = (e % RENDER_EVERY == 0) or (e >= EPISODES - 5)\n",
    "    \n",
    "    if render_this_episode:\n",
    "        print(f\"\\nüéÆ Rendering Episode {e+1}/{EPISODES}\")\n",
    "\n",
    "    for step in range(500): \n",
    "\n",
    "        # DQN control\n",
    "        action = agent.act(state)\n",
    "    \n",
    "        # Step environment\n",
    "        if uses_new_api:\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if step % 10 == 0 or done:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Try to get frame\n",
    "            frame = get_frame(env, render_method)\n",
    "            \n",
    "            if frame is not None:\n",
    "                # Display captured frame\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(frame)\n",
    "                plt.axis('off')\n",
    "                plt.title(f'Step: {step+1}')\n",
    "                plt.show()\n",
    "            else:\n",
    "                # Use manual visualization\n",
    "                fig = draw_cartpole_state(e, next_state, np.mean(scores[-10:]))\n",
    "                plt.show()\n",
    "                plt.close()            \n",
    "\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "        \n",
    "            scores.append(step + 1)\n",
    "            print(f\"Episode: {e+1}/{EPISODES}, Score: {step+1}, Œµ: {agent.epsilon:.3f}\")\n",
    "                \n",
    "            # Print progress bar\n",
    "            if (e + 1) % 10 == 0:\n",
    "                avg_score = np.mean(scores[-10:])\n",
    "                print(f\"üìä Last 10 episodes average: {avg_score:.1f}\")\n",
    "                    \n",
    "            break\n",
    "\n",
    "        # Train the agent\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Final average score (last 10 episodes): {np.mean(scores[-10:]):.1f}\")\n",
    "print(f\"Best score achieved: {max(scores)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Optional: Plot learning curve\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(scores, alpha=0.6, label='Episode scores')\n",
    "    \n",
    "    # Calculate rolling average\n",
    "    window = 10\n",
    "    rolling_avg = [np.mean(scores[max(0, i-window+1):i+1]) for i in range(len(scores))]\n",
    "    plt.plot(rolling_avg, linewidth=2, label=f'{window}-episode average')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('DQN Learning Progress on CartPole-v1')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Matplotlib not available for plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "code_explanation",
   "metadata": {},
   "source": [
    "## Understanding the Code\n",
    "\n",
    "### Key Components of Our DQN Implementation\n",
    "\n",
    "1. **Experience Replay Memory**\n",
    "   - `self.memory = deque(maxlen=2000)`: Stores past experiences\n",
    "   - Why? Learning from random samples breaks correlation between consecutive experiences\n",
    "   - Like studying with flashcards in random order vs. sequential order\n",
    "\n",
    "2. **Neural Network Architecture**\n",
    "   - Input layer: 4 neurons (state size)\n",
    "   - Hidden layers: Two layers with 24 neurons each\n",
    "   - Output layer: 2 neurons (one Q-value per action)\n",
    "   - Activation: ReLU for hidden layers, linear for output\n",
    "\n",
    "3. **Epsilon-Greedy Strategy**\n",
    "   - `self.epsilon = 1.0`: Start with 100% exploration\n",
    "   - `self.epsilon_decay = 0.995`: Reduce exploration by 0.5% each time\n",
    "   - `self.epsilon_min = 0.01`: Always keep 1% exploration\n",
    "\n",
    "4. **Learning Process**\n",
    "   - **Act**: Choose action using current policy\n",
    "   - **Step**: Execute action, observe reward and next state\n",
    "   - **Remember**: Store experience in memory\n",
    "   - **Replay**: Learn from random batch of past experiences\n",
    "\n",
    "### The Training Loop Explained\n",
    "\n",
    "For each episode:\n",
    "1. Reset environment to starting position\n",
    "2. Until pole falls or time limit:\n",
    "   - Agent observes current state\n",
    "   - Agent chooses action (push left or right)\n",
    "   - Environment responds with new state and reward\n",
    "   - Agent stores this experience\n",
    "   - Agent learns from past experiences (if enough collected)\n",
    "3. Track performance and adjust exploration rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical_tips",
   "metadata": {},
   "source": [
    "## Practical Tips and Common Challenges\n",
    "\n",
    "### Why Might Training Fail?\n",
    "\n",
    "1. **Too Much Exploration**: If epsilon stays high, agent keeps acting randomly\n",
    "2. **Too Little Exploration**: Agent gets stuck in suboptimal strategy\n",
    "3. **Poor Reward Design**: In our code, we give -10 for falling (encourages survival)\n",
    "4. **Insufficient Memory**: Small replay buffer might not capture diverse experiences\n",
    "\n",
    "### Hyperparameters to Tune\n",
    "\n",
    "- **Learning Rate** (0.001): How big are the weight updates?\n",
    "  - Too high: Unstable learning\n",
    "  - Too low: Slow learning\n",
    "\n",
    "- **Batch Size** (32): How many experiences to learn from at once?\n",
    "  - Larger: More stable but slower\n",
    "  - Smaller: Faster but noisier\n",
    "\n",
    "- **Network Size** (24 neurons √ó 2 layers): Model complexity\n",
    "  - Larger: Can learn complex patterns but might overfit\n",
    "  - Smaller: Faster, but might not capture all patterns\n",
    "\n",
    "### Signs of Successful Learning\n",
    "\n",
    "1. **Increasing Average Score**: Rolling average improves over time\n",
    "2. **Decreasing Variance**: Performance becomes more consistent\n",
    "3. **Strategic Behavior**: Agent develops recognizable patterns (e.g., gentle corrections)\n",
    "4. **Robust Performance**: Agent handles various starting conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensions",
   "metadata": {},
   "source": [
    "## Extensions and Advanced Topics\n",
    "\n",
    "### Improvements to Basic DQN\n",
    "\n",
    "1. **Double DQN**: Reduces overestimation of Q-values\n",
    "2. **Dueling DQN**: Separately estimates state value and action advantages\n",
    "3. **Prioritized Experience Replay**: Learn more from surprising experiences\n",
    "4. **Target Networks**: Separate network for generating targets (more stable)\n",
    "\n",
    "### Other RL Algorithms\n",
    "\n",
    "- **Policy Gradient Methods**: Directly learn the policy instead of Q-values\n",
    "- **Actor-Critic**: Combine value-based and policy-based methods\n",
    "- **PPO (Proximal Policy Optimization)**: State-of-the-art for continuous control\n",
    "- **SAC (Soft Actor-Critic)**: Works well for robotic control\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "1. **Robotics**: Teaching robots to walk, grasp, or manipulate objects\n",
    "2. **Game AI**: AlphaGo, OpenAI Five (Dota 2), StarCraft II\n",
    "3. **Resource Management**: Data center cooling, traffic light control\n",
    "4. **Finance**: Portfolio optimization, algorithmic trading\n",
    "5. **Healthcare**: Treatment recommendation, drug discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2712f10",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the fundamental concepts of reinforcement learning and implemented a Deep Q-Network (DQN) using TensorFlow and OpenAI Gym. \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **RL is Learning from Interaction**: Unlike supervised learning, RL agents learn by trying and receiving feedback\n",
    "2. **Markov Property Simplifies Problems**: Future depends only on present, not entire history\n",
    "3. **Q-Values Guide Decisions**: They tell us the long-term value of actions\n",
    "4. **Neural Networks Enable Scaling**: DQN can handle complex, continuous state spaces\n",
    "5. **Balance is Key**: Exploration vs exploitation, immediate vs future rewards\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- Try modifying hyperparameters and observe the effects\n",
    "- Implement improvements like target networks or double DQN\n",
    "- Apply DQN to other Gym environments (MountainCar, LunarLander)\n",
    "- Explore policy gradient methods for continuous action spaces\n",
    "\n",
    "Remember: RL is as much art as science. Experiment, observe, and iterate!\n",
    "\n",
    "### Exercises for Practice\n",
    "\n",
    "1. **Modify the Reward**: What happens if you give +0.1 for staying centered?\n",
    "2. **Change Network Architecture**: Try 3 hidden layers or different sizes\n",
    "3. **Adjust Exploration**: What if epsilon decays faster or slower?\n",
    "4. **Visualize Q-Values**: Plot how Q-values change during training\n",
    "5. **Compare Algorithms**: Implement a simple Q-table for CartPole (discretize states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}